{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity between 2 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine: [[0.81775832]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text1 = \"This is a foo bar sentence .\"\n",
    "text2 = \"This sentence is similar to a foo bar sentence .\"\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "data = tfidf.fit_transform([text1,text2])\n",
    "\n",
    "vector1 = tfidf.transform([text1]).toarray()\n",
    "vector2 = tfidf.transform([text2]).toarray()\n",
    "\n",
    "cosine = cosine_similarity(vector1, vector2)\n",
    "print(\"Cosine:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity between 2 Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between doc1 and doc2: [[0.23784716]]\n",
      "Cosine similarity between doc3 and doc3: [[0.06526311]]\n",
      "Cosine similarity between doc1 and doc3: [[0.06431176]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc1 = \"\"\"\n",
    "Coronavirus India State Wise Cases Latest Updates, Lockdown 5 News:  India has reported the highest single-day spike of 9,987 new COVID-19 cases and 331 deaths in the last 24 hours. The total number of coronavirus cases in the country now stands at 2,66,598. There are 1,29,917 active cases and 1,29,215 people have been cured/discharged/migrated, the Ministry of Health and Family Welfare latest data released on Tuesday said. The death toll has reached 7,466.  Meanwhile, Delhi Deputy Chief Minister Manish Sisodia has said that the city will 5.5 lakh coronavirus cases by July 31 of the current doubling rate continues. “5.5 lakh COVID-19 cases are expected in Delhi by July 31. We will need 80,000 beds by then,” Sisodia said after a meeting with Lieutenant Governor Anil Baijal. Sisodia also said that as per the Central government officials there is no community spread of coronavirus in Delhi so far.\n",
    "Coronavirus cases in India have breached the 2.66-lakh-mark and with over 1.29 lakh people being cured of the highly contagious disease, India’s recovery rate is said to be one of the best in the world. However, the World Health Organisation (WHO) has warned that the COVID-19 pandemic is ‘worsening’ globally. It said that while the situation in Europe, which witnessed thousands of deaths due to coronavirus in past few months, is improving, American, South Asian and African nations are witnessing high number of cases daily. WHO chief Tedros Adhanom Ghebreyesus said there are encouraging signs from several countries , but added that ‘complacency’ could be a big threat.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "A senior officer of the CBI in the Kolkata unit tested positive for COVID-19, officials said Tuesday.\n",
    "\n",
    "The officer, who is in a supervisory role looking after anti-corruption cases of the CBI’s Kolkata unit has become first such case among senior officers of the probe agency, they said.\n",
    "Sources in the agency said all possible medical help has been extended to the family.\n",
    "\n",
    "India has become fifth worst-hit nation by the COVID-19 pandemic after the US, Brazil, Russia and the UK, according to the Johns Hopkins University data.\n",
    "\n",
    "The country has registered over 9,000 coronavirus infection cases for the sixth day in a row taking the country tally to 2,66,598, according to Health Ministry data on Tuesday morning.\n",
    "Get live Stock Prices from BSE, NSE, US Market and latest NAV, portfolio of Mutual Funds, calculate your tax by Income Tax Calculator, know market’s Top Gainers, Top Losers & Best Equity Funds. Like us on Facebook and follow us on Twitter.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Dozens of scientists doing research funded by Mark Zuckerberg say Facebook should not be letting President Donald Trump use the social media platform to “spread both misinformation and incendiary statements.”\n",
    "\n",
    "The researchers, including 60 professors at leading US research institutions, wrote the Facebook CEO on Saturday asking Zuckerberg to “consider stricter policies on misinformation and incendiary language that harms people,” especially during the current turmoil over racial injustice.\n",
    "\n",
    "\n",
    " \n",
    "The letter calls the spread of “deliberate misinformation and divisive language” contrary to the researchers’ goals of using technology to prevent and eradicate disease, improve childhood education and reform the criminal justice system.\n",
    "\n",
    "\n",
    "Their mission “is antithetical to some of the stances that Facebook has been taking, so we’re encouraging them to be more on the side of truth and on the right side of history as we’ve said in the letter,” said Debora Marks of Harvard Medical School, one of three professors who organized it.\n",
    "\n",
    "The others are Martin Kampmann of the University of California-San Francisco and Jason Shepherd of the University of Utah. All have grants from a Chan Zuckerberg Initiative program working to prevent, cure and treat neurodegenerative disorders including Alzheimer’s and Parkinson’s disease.\n",
    "\n",
    "They said the letter had more than 160 signatories. Shepherd said about 10% are employees of foundations run by Zuckerberg and his wife, Priscilla Chan.\n",
    "\n",
    "\n",
    " \n",
    "The letter objects specifically to Zuckerberg’s decision not to at least flag as a violation of Facebook’s community standards Trump’s post that stated “when the looting starts, the shooting starts” in response to unrest in Minneapolis over the videotaped killing of George Floyd, a black man, by a white police officer. The letter’s authors called the post “a clear statement of inciting violence.”\n",
    "\n",
    "Twitter had both flagged and demoted a Trump tweet using the same language.\n",
    "\n",
    "In a statement, the Chan Zuckerberg Initiative noted that the philanthropic organization is separate from Facebook and said “we are grateful for our staff, partners and grantees” and “respect their right to voice their opinions, including on Facebook policies.”\n",
    "\n",
    "Some Facebook employees have publicly objected to Zuckerberg’s refusal to take down or label misleading or incendiary posts by Trump and other politicians. But Zuckerberg — who controls a majority of voting shares in the company — has so far refused.\n",
    "\n",
    "On Friday, Zuckerberg said in a post that he would review “potential options for handling violating or partially-violating content aside from the binary leave-it-up or take-it-down decisions”\n",
    "\n",
    "“I know many of you think we should have labeled the President’s posts in some way last week,” he wrote. “Our current policy is that if content is actually inciting violence, then the right mitigation is to take that content down — not let people continue seeing it behind a flag. There is no exception to this policy for politicians or newsworthiness.”\n",
    "\"\"\"\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "data = tfidf.fit_transform([doc1,doc2,doc3])\n",
    "\n",
    "vector1 = tfidf.transform([doc1]).toarray()\n",
    "vector2 = tfidf.transform([doc2]).toarray()\n",
    "vector3 = tfidf.transform([doc3]).toarray()\n",
    "\n",
    "cosine1 = cosine_similarity(vector1, vector2)\n",
    "print(\"Cosine similarity between doc1 and doc2:\", cosine1)\n",
    "cosine2 = cosine_similarity(vector2, vector3)\n",
    "print(\"Cosine similarity between doc3 and doc3:\", cosine2)\n",
    "cosine3 = cosine_similarity(vector1, vector3)\n",
    "print(\"Cosine similarity between doc1 and doc3:\", cosine3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After applying text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from autocorrect import spell\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "class Preprocess:\n",
    "    def __int__(self):\n",
    "        pass\n",
    "\n",
    "    def autospell(self,text):\n",
    "        \"\"\"\n",
    "        correct the spelling of the word.\n",
    "        \"\"\"\n",
    "        spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "        return \" \".join(spells)\n",
    "\n",
    "    def to_lower(self,text):\n",
    "        \"\"\"\n",
    "        :param text:\n",
    "        :return:\n",
    "            Converted text to lower case as in, converting \"Hello\" to \"hello\" or \"HELLO\" to \"hello\".\n",
    "        \"\"\"\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_numbers(self,text):\n",
    "        \"\"\"\n",
    "        take string input and return a clean text without numbers.\n",
    "        Use regex to discard the numbers.\n",
    "        \"\"\"\n",
    "        output = ''.join(c for c in text if not c.isdigit())\n",
    "        return output\n",
    "\n",
    "    def remove_punct(self,text):\n",
    "        \"\"\"\n",
    "        take string input and clean string without punctuations.\n",
    "        use regex to remove the punctuations.\n",
    "        \"\"\"\n",
    "        return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "    def remove_Tags(self,text):\n",
    "        \"\"\"\n",
    "        take string input and clean string without tags.\n",
    "        use regex to remove the html tags.\n",
    "        \"\"\"\n",
    "        cleaned_text = re.sub('<[^<]+?>', '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def sentence_tokenize(self,text):\n",
    "        \"\"\"\n",
    "        take string input and return list of sentences.\n",
    "        use nltk.sent_tokenize() to split the sentences.\n",
    "        \"\"\"\n",
    "        sent_list = []\n",
    "        for w in nltk.sent_tokenize(text):\n",
    "            sent_list.append(w)\n",
    "        return sent_list\n",
    "\n",
    "    def word_tokenize(self,text):\n",
    "        \"\"\"\n",
    "        :param text:\n",
    "        :return: list of words\n",
    "        \"\"\"\n",
    "        return [w for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]\n",
    "\n",
    "    def remove_stopwords(self,sentence):\n",
    "        \"\"\"\n",
    "        removes all the stop words like \"is,the,a, etc.\"\n",
    "        \"\"\"\n",
    "        stop_words = stopwords.words('english')\n",
    "        return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n",
    "\n",
    "    def stem(self,text):\n",
    "        \"\"\"\n",
    "        :param word_tokens:\n",
    "        :return: list of words\n",
    "        \"\"\"\n",
    "        stemmed_word = [snowball_stemmer.stem(word) for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "        return \" \".join(stemmed_word)\n",
    "\n",
    "    def lemmatize(self,text):\n",
    "        lemmatized_word = [wordnet_lemmatizer.lemmatize(word)for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "        return \" \".join(lemmatized_word)\n",
    "\n",
    "\n",
    "    def preprocess(self,text):\n",
    "        lower_text = self.to_lower(text)\n",
    "        sentence_tokens = self.sentence_tokenize(lower_text)\n",
    "        word_list = []\n",
    "        for each_sent in sentence_tokens:\n",
    "            lemmatizzed_sent = self.lemmatize(each_sent)\n",
    "            clean_text = self.remove_numbers(lemmatizzed_sent)\n",
    "            clean_text = self.remove_punct(clean_text)\n",
    "            clean_text = self.remove_Tags(clean_text)\n",
    "            clean_text = self.remove_stopwords(clean_text)\n",
    "            word_tokens = self.word_tokenize(clean_text)\n",
    "            for i in word_tokens:\n",
    "                word_list.append(i)\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between doc1 and doc2: [[0.28282452]]\n",
      "Cosine similarity between doc3 and doc3: [[0.07791204]]\n",
      "Cosine similarity between doc1 and doc3: [[0.07407567]]\n"
     ]
    }
   ],
   "source": [
    "pr = Preprocess()\n",
    "doc1_cleaned = \" \".join(pr.preprocess(doc1))\n",
    "doc2_cleaned = \" \".join(pr.preprocess(doc2))\n",
    "doc3_cleaned = \" \".join(pr.preprocess(doc3))\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit_transform([doc1_cleaned, doc2_cleaned, doc3_cleaned])\n",
    "\n",
    "vector1 = tfidf.transform([doc1_cleaned]).toarray()\n",
    "vector2 = tfidf.transform([doc2_cleaned]).toarray()\n",
    "vector3 = tfidf.transform([doc3_cleaned]).toarray()\n",
    "\n",
    "cosine1 = cosine_similarity(vector1, vector2)\n",
    "print(\"Cosine similarity between doc1 and doc2:\", cosine1)\n",
    "cosine2 = cosine_similarity(vector2, vector3)\n",
    "print(\"Cosine similarity between doc3 and doc3:\", cosine2)\n",
    "cosine3 = cosine_similarity(vector1, vector3)\n",
    "print(\"Cosine similarity between doc1 and doc3:\", cosine3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b>\n",
    "As you can see there's slight score change with and without the pre-processing. <br>\n",
    "Before pre-processing:<br>\n",
    "Cosine similarity between doc1 and doc2: [[0.23784716]]<br>\n",
    "Cosine similarity between doc3 and doc3: [[0.06526311]]<br>\n",
    "Cosine similarity between doc1 and doc3: [[0.06431176]]<br><br>\n",
    "After pre-processing: <br>\n",
    "Cosine similarity between doc1 and doc2: [[0.28282452]]<br>\n",
    "Cosine similarity between doc3 and doc3: [[0.07791204]]<br>\n",
    "Cosine similarity between doc1 and doc3: [[0.07407567]]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
